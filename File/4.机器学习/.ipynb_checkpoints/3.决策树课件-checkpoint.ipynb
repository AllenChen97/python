{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 经验熵的python实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9709505944546686\n"
     ]
    }
   ],
   "source": [
    "from math import log\n",
    "def createDataSet():\n",
    "    dataSet = [\n",
    "            [0, 0, 0, 0, 'no'],         #数据集\n",
    "            [0, 0, 0, 1, 'no'],\n",
    "            [0, 1, 0, 1, 'yes'],\n",
    "            [0, 1, 1, 0, 'yes'],\n",
    "            [0, 0, 0, 0, 'no'],\n",
    "            [1, 0, 0, 0, 'no'],\n",
    "            [1, 0, 0, 1, 'no'],\n",
    "            [1, 1, 1, 1, 'yes'],\n",
    "            [1, 0, 1, 2, 'yes'],\n",
    "            [1, 0, 1, 2, 'yes'],\n",
    "            [2, 0, 1, 2, 'yes'],\n",
    "            [2, 0, 1, 1, 'yes'],\n",
    "            [2, 1, 0, 1, 'yes'],\n",
    "            [2, 1, 0, 2, 'yes'],\n",
    "            [2, 0, 0, 0, 'no']]\n",
    "    labels = ['不放贷', '放贷']             #分类属性\n",
    "    return dataSet, labels                #返回数据集和分类属性\n",
    "\n",
    "def calcShannonEnt(dataSet):\n",
    "    numEntires = len(dataSet)                        #返回数据集的行数\n",
    "    labelCounts = {}                                #保存每个标签(Label)出现次数的字典\n",
    "    for featVec in dataSet:                            #对每组特征向量进行统计\n",
    "        currentLabel = featVec[-1]                    #提取标签(Label)信息   \n",
    "        labelCounts[currentLabel]= labelCounts.get(currentLabel,0)+1\n",
    "    shannonEnt = 0.0                                #经验熵(香农熵)\n",
    "    for key in labelCounts:                            #计算香农熵\n",
    "        prob = float(labelCounts[key]) / numEntires    #选择该标签(Label)的概率\n",
    "        shannonEnt -= prob * log(prob, 2)            #利用公式计算\n",
    "    return shannonEnt                                #返回经验熵(香农熵)\n",
    " \n",
    "if __name__ == '__main__':\n",
    "    dataSet, features = createDataSet()\n",
    "    #print(dataSet)\n",
    "    print(calcShannonEnt(dataSet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 计算信息增益"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第0个特征的增益为0.083\n",
      "第1个特征的增益为0.324\n",
      "第2个特征的增益为0.420\n",
      "第3个特征的增益为0.363\n",
      "最优特征索引值:2\n"
     ]
    }
   ],
   "source": [
    "#计算信息增益\n",
    "from math import log\n",
    "#H(D)\n",
    "def calcShannonEnt(dataSet):\n",
    "    numEntires = len(dataSet)                        #返回数据集的行数\n",
    "    labelCounts = {}                                #保存每个标签(Label)出现次数的字典\n",
    "    for featVec in dataSet:                            #对每组特征向量进行统计\n",
    "        currentLabel = featVec[-1]                    #提取标签(Label)信息\n",
    "        if currentLabel not in labelCounts.keys():    #如果标签(Label)没有放入统计次数的字典,添加进去\n",
    "            labelCounts[currentLabel] = 0\n",
    "        labelCounts[currentLabel] += 1                #Label计数\n",
    "    shannonEnt = 0.0                                #经验熵(香农熵)\n",
    "    for key in labelCounts:                            #计算香农熵\n",
    "        prob = float(labelCounts[key]) / numEntires    #选择该标签(Label)的概率\n",
    "        shannonEnt -= prob * log(prob, 2)            #利用公式计算\n",
    "    return shannonEnt                                #返回经验熵(香农熵)\n",
    " \n",
    "def createDataSet():\n",
    "    dataSet = [\n",
    "            [0, 0, 0, 0, 'no'],                        #数据集\n",
    "            [0, 0, 0, 1, 'no'],\n",
    "            [0, 1, 0, 1, 'yes'],\n",
    "            [0, 1, 1, 0, 'yes'],\n",
    "            [0, 0, 0, 0, 'no'],\n",
    "            [1, 0, 0, 0, 'no'],\n",
    "            [1, 0, 0, 1, 'no'],\n",
    "            [1, 1, 1, 1, 'yes'],\n",
    "            [1, 0, 1, 2, 'yes'],\n",
    "            [1, 0, 1, 2, 'yes'],\n",
    "            [2, 0, 1, 2, 'yes'],\n",
    "            [2, 0, 1, 1, 'yes'],\n",
    "            [2, 1, 0, 1, 'yes'],\n",
    "            [2, 1, 0, 2, 'yes'],\n",
    "            [2, 0, 0, 0, 'no']]\n",
    "    labels = ['不放贷', '放贷']            #分类属性\n",
    "    return dataSet, labels                             #返回数据集和分类属性\n",
    " \n",
    "def splitDataSet(dataSet, axis, value):       #value表示选择第几个特征    \n",
    "    retDataSet = []                                        #创建返回的数据集列表\n",
    "    for featVec in dataSet:                             #遍历数据集\n",
    "        if featVec[axis] == value:\n",
    "            reducedFeatVec = featVec[:axis]                #去掉axis特征\n",
    "            reducedFeatVec.extend(featVec[axis+1:])     #将符合条件的添加到返回的数据集\n",
    "            retDataSet.append(reducedFeatVec)\n",
    "            #reducedFeatVec = featVec[axis+1:]\n",
    "    return retDataSet                                      #返回划分后的数据集\n",
    "\n",
    "\n",
    "def chooseBestFeatureToSplit(dataSet):\n",
    "    numFeatures = len(dataSet[0]) - 1      #特征数量\n",
    "    baseEntropy = calcShannonEnt(dataSet)#H(D)  #计算数据集的香农熵，0.971\n",
    "    bestInfoGain = 0.0                                  #信息增益\n",
    "    bestFeature = -1                                    #最优特征的索引值\n",
    "    for i in range(numFeatures):                         #遍历所有特征\n",
    "        featList = [example[i] for example in dataSet]  #获取dataSet的第i个所有特征\n",
    "        uniqueVals = set(featList)                         #创建set集合{},元素不可重复\n",
    "        newEntropy = 0.0                                  #经验条件熵\n",
    "        for value in uniqueVals:                         #计算信息增益\n",
    "            subDataSet = splitDataSet(dataSet, i, value)         #subDataSet划分后的子集\n",
    "            prob = len(subDataSet) / float(len(dataSet))           #计算子集的概率\n",
    "            newEntropy += prob * calcShannonEnt(subDataSet)     #根据公式计算经验条件熵\n",
    "        infoGain = baseEntropy - newEntropy                     #信息增益\n",
    "        print(\"第%d个特征的增益为%.3f\" % (i, infoGain))            #打印每个特征的信息增益\n",
    "        if (infoGain > bestInfoGain):                             #计算信息增益\n",
    "            bestInfoGain = infoGain                             #更新信息增益，找到最大的信息增益\n",
    "            bestFeature = i                                     #记录信息增益最大的特征的索引值\n",
    "    return bestFeature                                             #返回信息增益最大的特征的索引值\n",
    " \n",
    "if __name__ == '__main__':\n",
    "    dataSet, features = createDataSet()\n",
    "    print(\"最优特征索引值:\" + str(chooseBestFeatureToSplit(dataSet)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 决策树构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'有自己的房子': {0: {'有工作': {0: 'no', 1: 'yes'}}, 1: 'yes'}}\n"
     ]
    }
   ],
   "source": [
    "from math import log\n",
    "import operator\n",
    "\n",
    "#计算H(D)\n",
    "def calcShannonEnt(dataSet):\n",
    "    numEntires = len(dataSet)                        #返回数据集的行数\n",
    "    labelCounts = {}                                #保存每个标签(Label)出现次数的字典\n",
    "    for featVec in dataSet:                            #对每组特征向量进行统计\n",
    "        currentLabel = featVec[-1]                    #提取标签(Label)信息\n",
    "        if currentLabel not in labelCounts.keys():    #如果标签(Label)没有放入统计次数的字典,添加进去\n",
    "            labelCounts[currentLabel] = 0\n",
    "        labelCounts[currentLabel] += 1                #Label计数\n",
    "    shannonEnt = 0.0                                #经验熵(香农熵)\n",
    "    for key in labelCounts:                            #计算香农熵\n",
    "        prob = float(labelCounts[key]) / numEntires    #选择该标签(Label)的概率\n",
    "        shannonEnt -= prob * log(prob, 2)            #利用公式计算\n",
    "    return shannonEnt                                #返回经验熵(香农熵)\n",
    " \n",
    "def createDataSet():\n",
    "    dataSet = [\n",
    "            [0, 0, 0, 0, 'no'],                        #数据集\n",
    "            [0, 0, 0, 1, 'no'],\n",
    "            [0, 1, 0, 1, 'yes'],\n",
    "            [0, 1, 1, 0, 'yes'],\n",
    "            [0, 0, 0, 0, 'no'],\n",
    "            [1, 0, 0, 0, 'no'],\n",
    "            [1, 0, 0, 1, 'no'],\n",
    "            [1, 1, 1, 1, 'yes'],\n",
    "            [1, 0, 1, 2, 'yes'],\n",
    "            [1, 0, 1, 2, 'yes'],\n",
    "            [2, 0, 1, 2, 'yes'],\n",
    "            [2, 0, 1, 1, 'yes'],\n",
    "            [2, 1, 0, 1, 'yes'],\n",
    "            [2, 1, 0, 2, 'yes'],\n",
    "            [2, 0, 0, 0, 'no']]\n",
    "    labels = ['年龄', '有工作', '有自己的房子', '信贷情况']        #特征标签\n",
    "    return dataSet, labels                             #返回数据集和分类属性\n",
    " \n",
    "\n",
    "def splitDataSet(dataSet, axis, value):       \n",
    "    retDataSet = []                                        #创建返回的数据集列表\n",
    "    for featVec in dataSet:                             #遍历数据集\n",
    "        if featVec[axis] == value:\n",
    "            reducedFeatVec = featVec[:axis]                #去掉axis特征\n",
    "            reducedFeatVec.extend(featVec[axis+1:])     #将符合条件的添加到返回的数据集\n",
    "            retDataSet.append(reducedFeatVec)\n",
    "    return retDataSet                                      #返回划分后的数据集\n",
    " \n",
    "\n",
    "def chooseBestFeatureToSplit(dataSet):\n",
    "    numFeatures = len(dataSet[0]) - 1                    #特征数量\n",
    "    baseEntropy = calcShannonEnt(dataSet)                 #计算数据集的香农熵\n",
    "    bestInfoGain = 0.0                                  #信息增益\n",
    "    bestFeature = -1                                    #最优特征的索引值\n",
    "    for i in range(numFeatures):                         #遍历所有特征\n",
    "        #获取dataSet的第i个所有特征\n",
    "        featList = [example[i] for example in dataSet]\n",
    "        uniqueVals = set(featList)                         #创建set集合{},元素不可重复\n",
    "        newEntropy = 0.0                                  #经验条件熵\n",
    "        for value in uniqueVals:                         #计算信息增益\n",
    "            subDataSet = splitDataSet(dataSet, i, value)         #subDataSet划分后的子集\n",
    "            prob = len(subDataSet) / float(len(dataSet))           #计算子集的概率\n",
    "            newEntropy += prob * calcShannonEnt(subDataSet)     #根据公式计算经验条件熵\n",
    "        infoGain = baseEntropy - newEntropy                     #信息增益\n",
    "        # print(\"第%d个特征的增益为%.3f\" % (i, infoGain))            #打印每个特征的信息增益\n",
    "        if (infoGain > bestInfoGain):                             #计算信息增益\n",
    "            bestInfoGain = infoGain                             #更新信息增益，找到最大的信息增益\n",
    "            bestFeature = i                                     #记录信息增益最大的特征的索引值\n",
    "    return bestFeature                                             #返回信息增益最大的特征的索引值\n",
    " \n",
    "\n",
    "def majorityCnt(classList):\n",
    "    classCount = {}\n",
    "    for vote in classList:                                        #统计classList中每个元素出现的次数\n",
    "        classCount[vote]=classCount.get(vote,0)+1       \n",
    "    sortedClassCount = sorted(classCount.items(), key = operator.itemgetter(1), reverse = True)        #根据字典的值降序排序\n",
    "    return sortedClassCount[0][0]                                #返回classList中出现次数最多的元素\n",
    "\n",
    "#构建决策树\n",
    "def createTree(dataSet, labels, featLabels):\n",
    "    classList = [example[-1] for example in dataSet]               #取分类标签(是否放贷:yes or no)\n",
    "    if classList.count(classList[0]) == len(classList):            #如果类别完全相同则停止继续划分\n",
    "        return classList[0]\n",
    "    if len(dataSet[0]) == 1:                                    #遍历完所有特征时返回出现次数最多的类标签\n",
    "        return majorityCnt(classList)\n",
    "    bestFeat = chooseBestFeatureToSplit(dataSet)                #选择最优特征\n",
    "    bestFeatLabel = labels[bestFeat]                            #最优特征的标签\n",
    "    myTree = {bestFeatLabel:{}}                                    #根据最优特征的标签生成树\n",
    "    featLabels.append(bestFeatLabel)\n",
    "    del(labels[bestFeat])                                        #删除已经使用特征标签\n",
    "    featValues = [example[bestFeat] for example in dataSet]        #得到训练集中所有最优特征的属性值\n",
    "    uniqueVals = set(featValues)                                #去掉重复的属性值\n",
    "    for value in uniqueVals:                                    #遍历特征，创建决策树。                       \n",
    "        myTree[bestFeatLabel][value] = createTree(splitDataSet(dataSet, bestFeat, value), labels, featLabels)\n",
    "    return myTree\n",
    " \n",
    "if __name__ == '__main__':\n",
    "    dataSet, labels = createDataSet()\n",
    "    featLabels = []#可变序列\n",
    "    myTree = createTree(dataSet, labels, featLabels)\n",
    "    print(myTree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def add1(a):\n",
    "    a=a+1\n",
    "    return a\n",
    "a=1\n",
    "add1(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=[]\n",
    "def add1(a):\n",
    "    a.append(1)\n",
    "    return a\n",
    "#可变序列，调用函数会改变全局变量\n",
    "add1(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 利用决策树执行分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['是否有自己的房子', '是否有工作']\n",
      "放贷\n"
     ]
    }
   ],
   "source": [
    "from math import log\n",
    "import operator\n",
    " \n",
    "def calcShannonEnt(dataSet):\n",
    "    numEntires = len(dataSet)                        #返回数据集的行数\n",
    "    labelCounts = {}                                #保存每个标签(Label)出现次数的字典\n",
    "    for featVec in dataSet:                            #对每组特征向量进行统计\n",
    "        currentLabel = featVec[-1]                    #提取标签(Label)信息\n",
    "        if currentLabel not in labelCounts.keys():    #如果标签(Label)没有放入统计次数的字典,添加进去\n",
    "            labelCounts[currentLabel] = 0\n",
    "        labelCounts[currentLabel] += 1                #Label计数\n",
    "    shannonEnt = 0.0                                #经验熵(香农熵)\n",
    "    for key in labelCounts:                            #计算香农熵\n",
    "        prob = float(labelCounts[key]) / numEntires    #选择该标签(Label)的概率\n",
    "        shannonEnt -= prob * log(prob, 2)            #利用公式计算\n",
    "    return shannonEnt                                #返回经验熵(香农熵)\n",
    " \n",
    "\n",
    "def createDataSet():\n",
    "    dataSet = [[0, 0, 0, 0, 'no'],                        #数据集\n",
    "            [0, 0, 0, 1, 'no'],\n",
    "            [0, 1, 0, 1, 'yes'],\n",
    "            [0, 1, 1, 0, 'yes'],\n",
    "            [0, 0, 0, 0, 'no'],\n",
    "            [1, 0, 0, 0, 'no'],\n",
    "            [1, 0, 0, 1, 'no'],\n",
    "            [1, 1, 1, 1, 'yes'],\n",
    "            [1, 0, 1, 2, 'yes'],\n",
    "            [1, 0, 1, 2, 'yes'],\n",
    "            [2, 0, 1, 2, 'yes'],\n",
    "            [2, 0, 1, 1, 'yes'],\n",
    "            [2, 1, 0, 1, 'yes'],\n",
    "            [2, 1, 0, 2, 'yes'],\n",
    "            [2, 0, 0, 0, 'no']]\n",
    "    labels = ['年龄', '是否有工作', '是否有自己的房子', '信贷情况']        #特征标签\n",
    "    return dataSet, labels                             #返回数据集和分类属性\n",
    "\n",
    "def splitDataSet(dataSet, axis, value):       \n",
    "    retDataSet = []                                        #创建返回的数据集列表\n",
    "    for featVec in dataSet:                             #遍历数据集\n",
    "        if featVec[axis] == value:\n",
    "            reducedFeatVec = featVec[:axis]                #去掉axis特征\n",
    "            reducedFeatVec.extend(featVec[axis+1:])     #将符合条件的添加到返回的数据集\n",
    "            retDataSet.append(reducedFeatVec)\n",
    "    return retDataSet                                      #返回划分后的数据集\n",
    " \n",
    "\n",
    "def chooseBestFeatureToSplit(dataSet):\n",
    "    numFeatures = len(dataSet[0]) - 1                    #特征数量\n",
    "    baseEntropy = calcShannonEnt(dataSet)                 #计算数据集的香农熵\n",
    "    bestInfoGain = 0.0                                  #信息增益\n",
    "    bestFeature = -1                                    #最优特征的索引值\n",
    "    for i in range(numFeatures):                         #遍历所有特征\n",
    "        #获取dataSet的第i个所有特征\n",
    "        featList = [example[i] for example in dataSet]\n",
    "        uniqueVals = set(featList)                         #创建set集合{},元素不可重复\n",
    "        newEntropy = 0.0                                  #经验条件熵\n",
    "        for value in uniqueVals:                         #计算信息增益\n",
    "            subDataSet = splitDataSet(dataSet, i, value)         #subDataSet划分后的子集\n",
    "            prob = len(subDataSet) / float(len(dataSet))           #计算子集的概率\n",
    "            newEntropy += prob * calcShannonEnt(subDataSet)     #根据公式计算经验条件熵\n",
    "        infoGain = baseEntropy - newEntropy                     #信息增益\n",
    "        # print(\"第%d个特征的增益为%.3f\" % (i, infoGain))            #打印每个特征的信息增益\n",
    "        if (infoGain > bestInfoGain):                             #计算信息增益\n",
    "            bestInfoGain = infoGain                             #更新信息增益，找到最大的信息增益\n",
    "            bestFeature = i                                     #记录信息增益最大的特征的索引值\n",
    "    return bestFeature                                             #返回信息增益最大的特征的索引值\n",
    " \n",
    "\n",
    "def majorityCnt(classList):\n",
    "    classCount = {}\n",
    "    for vote in classList:                                        #统计classList中每个元素出现的次数\n",
    "        if vote not in classCount.keys():classCount[vote] = 0   \n",
    "        classCount[vote] += 1\n",
    "    sortedClassCount = sorted(classCount.items(), key = operator.itemgetter(1), reverse = True)        #根据字典的值降序排序\n",
    "    return sortedClassCount[0][0]                                #返回classList中出现次数最多的元素\n",
    "\n",
    "def createTree(dataSet, labels, featLabels):\n",
    "    classList = [example[-1] for example in dataSet]            #取分类标签(是否放贷:yes or no)\n",
    "    if classList.count(classList[0]) == len(classList):            #如果类别完全相同则停止继续划分\n",
    "        return classList[0]\n",
    "    if len(dataSet[0]) == 1:                                    #遍历完所有特征时返回出现次数最多的类标签\n",
    "        return majorityCnt(classList)\n",
    "    bestFeat = chooseBestFeatureToSplit(dataSet)                #选择最优特征\n",
    "    bestFeatLabel = labels[bestFeat]                            #最优特征的标签\n",
    "    featLabels.append(bestFeatLabel)\n",
    "    myTree = {bestFeatLabel:{}}                                    #根据最优特征的标签生成树\n",
    "    del(labels[bestFeat])                                        #删除已经使用特征标签\n",
    "    featValues = [example[bestFeat] for example in dataSet]        #得到训练集中所有最优特征的属性值\n",
    "    uniqueVals = set(featValues)                                #去掉重复的属性值\n",
    "    for value in uniqueVals:                                    #遍历特征，创建决策树。                       \n",
    "        myTree[bestFeatLabel][value] = createTree(splitDataSet(dataSet, bestFeat, value), labels, featLabels)\n",
    "    return myTree\n",
    "\n",
    "def classify(inputTree, featLabels, testVec):\n",
    "    firstStr=list(inputTree)[0]#获取决策树结点\n",
    "    secondDict = inputTree[firstStr]                          #键值对，下一个字典\n",
    "    featIndex = featLabels.index(firstStr)                                               \n",
    "    for key in secondDict.keys():\n",
    "        if testVec[featIndex] == key:\n",
    "            if type(secondDict[key]).__name__ == 'dict':\n",
    "                classLabel = classify(secondDict[key], featLabels, testVec)\n",
    "            else: classLabel = secondDict[key]\n",
    "    return classLabel\n",
    " \n",
    "if __name__ == '__main__':\n",
    "    dataSet, labels = createDataSet()\n",
    "    featLabels = []\n",
    "    myTree = createTree(dataSet, labels, featLabels)#注意featlabels是列表，会修改\n",
    "    print(featLabels)\n",
    "    testVec = [1,0]    #测试数据,第一个是房子，第二个是工作，没房子，有工作                                    \n",
    "    result = classify(myTree, featLabels, testVec)\n",
    "    if result == 'yes':\n",
    "        print('放贷')\n",
    "    if result == 'no':\n",
    "        print('不放贷')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 决策树的存储"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#决策树的存储\n",
    "import pickle\n",
    "def storeTree(inputTree, filename):\n",
    "    with open(filename, 'wb') as fw:\n",
    "        pickle.dump(inputTree, fw)\n",
    "if __name__ == '__main__':\n",
    "    myTree = {'有自己的房子': {0: {'有工作': {0: 'no', 1: 'yes'}}, 1: 'yes'}}\n",
    "    storeTree(myTree, 'classifierStorage0.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'有自己的房子': {0: {'有工作': {0: 'no', 1: 'yes'}}, 1: 'yes'}}\n"
     ]
    }
   ],
   "source": [
    "#取决策树\n",
    "def grabTree(filename):\n",
    "    fr = open(filename, 'rb')\n",
    "    return pickle.load(fr)\n",
    " \n",
    "if __name__ == '__main__':\n",
    "    myTree = grabTree('classifierStorage0.txt')\n",
    "    print(myTree)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用决策树预测眼镜类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('C:\\\\Users\\\\chen\\\\File')\n",
    "import pandas as pd\n",
    "from sklearn import tree\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "data=pd.read_table('lenses.txt',names=['age', 'prescript', 'astigmatic', 'tearRate','label'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>prescript</th>\n",
       "      <th>astigmatic</th>\n",
       "      <th>tearRate</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>young</td>\n",
       "      <td>myope</td>\n",
       "      <td>no</td>\n",
       "      <td>reduced</td>\n",
       "      <td>no lenses</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>young</td>\n",
       "      <td>myope</td>\n",
       "      <td>no</td>\n",
       "      <td>normal</td>\n",
       "      <td>soft</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>young</td>\n",
       "      <td>myope</td>\n",
       "      <td>yes</td>\n",
       "      <td>reduced</td>\n",
       "      <td>no lenses</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>young</td>\n",
       "      <td>myope</td>\n",
       "      <td>yes</td>\n",
       "      <td>normal</td>\n",
       "      <td>hard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>young</td>\n",
       "      <td>hyper</td>\n",
       "      <td>no</td>\n",
       "      <td>reduced</td>\n",
       "      <td>no lenses</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>young</td>\n",
       "      <td>hyper</td>\n",
       "      <td>no</td>\n",
       "      <td>normal</td>\n",
       "      <td>soft</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>young</td>\n",
       "      <td>hyper</td>\n",
       "      <td>yes</td>\n",
       "      <td>reduced</td>\n",
       "      <td>no lenses</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>young</td>\n",
       "      <td>hyper</td>\n",
       "      <td>yes</td>\n",
       "      <td>normal</td>\n",
       "      <td>hard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>pre</td>\n",
       "      <td>myope</td>\n",
       "      <td>no</td>\n",
       "      <td>reduced</td>\n",
       "      <td>no lenses</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>pre</td>\n",
       "      <td>myope</td>\n",
       "      <td>no</td>\n",
       "      <td>normal</td>\n",
       "      <td>soft</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>pre</td>\n",
       "      <td>myope</td>\n",
       "      <td>yes</td>\n",
       "      <td>reduced</td>\n",
       "      <td>no lenses</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>pre</td>\n",
       "      <td>myope</td>\n",
       "      <td>yes</td>\n",
       "      <td>normal</td>\n",
       "      <td>hard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>pre</td>\n",
       "      <td>hyper</td>\n",
       "      <td>no</td>\n",
       "      <td>reduced</td>\n",
       "      <td>no lenses</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>pre</td>\n",
       "      <td>hyper</td>\n",
       "      <td>no</td>\n",
       "      <td>normal</td>\n",
       "      <td>soft</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>pre</td>\n",
       "      <td>hyper</td>\n",
       "      <td>yes</td>\n",
       "      <td>reduced</td>\n",
       "      <td>no lenses</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>pre</td>\n",
       "      <td>hyper</td>\n",
       "      <td>yes</td>\n",
       "      <td>normal</td>\n",
       "      <td>no lenses</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>presbyopic</td>\n",
       "      <td>myope</td>\n",
       "      <td>no</td>\n",
       "      <td>reduced</td>\n",
       "      <td>no lenses</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>presbyopic</td>\n",
       "      <td>myope</td>\n",
       "      <td>no</td>\n",
       "      <td>normal</td>\n",
       "      <td>no lenses</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>presbyopic</td>\n",
       "      <td>myope</td>\n",
       "      <td>yes</td>\n",
       "      <td>reduced</td>\n",
       "      <td>no lenses</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>presbyopic</td>\n",
       "      <td>myope</td>\n",
       "      <td>yes</td>\n",
       "      <td>normal</td>\n",
       "      <td>hard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>presbyopic</td>\n",
       "      <td>hyper</td>\n",
       "      <td>no</td>\n",
       "      <td>reduced</td>\n",
       "      <td>no lenses</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>presbyopic</td>\n",
       "      <td>hyper</td>\n",
       "      <td>no</td>\n",
       "      <td>normal</td>\n",
       "      <td>soft</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>presbyopic</td>\n",
       "      <td>hyper</td>\n",
       "      <td>yes</td>\n",
       "      <td>reduced</td>\n",
       "      <td>no lenses</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>presbyopic</td>\n",
       "      <td>hyper</td>\n",
       "      <td>yes</td>\n",
       "      <td>normal</td>\n",
       "      <td>no lenses</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           age prescript astigmatic tearRate      label\n",
       "0        young     myope         no  reduced  no lenses\n",
       "1        young     myope         no   normal       soft\n",
       "2        young     myope        yes  reduced  no lenses\n",
       "3        young     myope        yes   normal       hard\n",
       "4        young     hyper         no  reduced  no lenses\n",
       "5        young     hyper         no   normal       soft\n",
       "6        young     hyper        yes  reduced  no lenses\n",
       "7        young     hyper        yes   normal       hard\n",
       "8          pre     myope         no  reduced  no lenses\n",
       "9          pre     myope         no   normal       soft\n",
       "10         pre     myope        yes  reduced  no lenses\n",
       "11         pre     myope        yes   normal       hard\n",
       "12         pre     hyper         no  reduced  no lenses\n",
       "13         pre     hyper         no   normal       soft\n",
       "14         pre     hyper        yes  reduced  no lenses\n",
       "15         pre     hyper        yes   normal  no lenses\n",
       "16  presbyopic     myope         no  reduced  no lenses\n",
       "17  presbyopic     myope         no   normal  no lenses\n",
       "18  presbyopic     myope        yes  reduced  no lenses\n",
       "19  presbyopic     myope        yes   normal       hard\n",
       "20  presbyopic     hyper         no  reduced  no lenses\n",
       "21  presbyopic     hyper         no   normal       soft\n",
       "22  presbyopic     hyper        yes  reduced  no lenses\n",
       "23  presbyopic     hyper        yes   normal  no lenses"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "#编码\n",
    "lenses_target=data['label']\n",
    "lenses_pd=data[['age', 'prescript', 'astigmatic', 'tearRate']]\n",
    "le = LabelEncoder()                                                        #创建LabelEncoder()对象，用于序列化           \n",
    "for col in lenses_pd.columns:                                            #序列化\n",
    "    lenses_pd[col] = le.fit_transform(lenses_pd[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>prescript</th>\n",
       "      <th>astigmatic</th>\n",
       "      <th>tearRate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    age  prescript  astigmatic  tearRate\n",
       "0     2          1           0         1\n",
       "1     2          1           0         0\n",
       "2     2          1           1         1\n",
       "3     2          1           1         0\n",
       "4     2          0           0         1\n",
       "5     2          0           0         0\n",
       "6     2          0           1         1\n",
       "7     2          0           1         0\n",
       "8     0          1           0         1\n",
       "9     0          1           0         0\n",
       "10    0          1           1         1\n",
       "11    0          1           1         0\n",
       "12    0          0           0         1\n",
       "13    0          0           0         0\n",
       "14    0          0           1         1\n",
       "15    0          0           1         0\n",
       "16    1          1           0         1\n",
       "17    1          1           0         0\n",
       "18    1          1           1         1\n",
       "19    1          1           1         0\n",
       "20    1          0           0         1\n",
       "21    1          0           0         0\n",
       "22    1          0           1         1\n",
       "23    1          0           1         0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lenses_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Source.gv.pdf'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import graphviz\n",
    "import os\n",
    "os.chdir(r'C:\\graphviz-2.38\\release\\bin')\n",
    "clf = tree.DecisionTreeClassifier(max_depth = 4)                        #创建DecisionTreeClassifier()类\n",
    "clf = clf.fit(lenses_pd, lenses_target)                    #使用数据，构建决策树\n",
    "dota_data=tree.export_graphviz(clf, out_file = None,                            #绘制决策树\n",
    "                    feature_names = lenses_pd.keys(),\n",
    "                    class_names = clf.classes_,\n",
    "                    filled=True, rounded=True,\n",
    "                    special_characters=True)   \n",
    "graph=graphviz.Source(dota_data)\n",
    "graph.view()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Source.gv.pdf'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#完整代码\n",
    "import os\n",
    "os.chdir('C:\\\\Users\\\\chen\\\\File')\n",
    "import graphviz\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "data=pd.read_table('lenses.txt',names=['age', 'prescript', 'astigmatic', 'tearRate','label'] )\n",
    "#编码\n",
    "lenses_target=data['label']\n",
    "lenses_pd=data[['age', 'prescript', 'astigmatic', 'tearRate']]\n",
    "le = LabelEncoder()                                                        #创建LabelEncoder()对象，用于序列化           \n",
    "for col in lenses_pd.columns:                                            #序列化\n",
    "    lenses_pd[col] = le.fit_transform(lenses_pd[col])\n",
    "#构建决策树\n",
    "\n",
    "clf = tree.DecisionTreeClassifier(max_depth = 4)                        #创建DecisionTreeClassifier()类\n",
    "clf = clf.fit(lenses_pd, lenses_target)                    #使用数据，构建决策树\n",
    "dota_data=tree.export_graphviz(clf, out_file = None,                            #绘制决策树\n",
    "                    feature_names = lenses_pd.keys(),\n",
    "                    class_names = clf.classes_,\n",
    "                    filled=True, rounded=True,\n",
    "                    special_characters=True) \n",
    "os.chdir(r'C:\\graphviz-2.38\\release\\bin')\n",
    "graph=graphviz.Source(dota_data)\n",
    "graph.view()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hard']\n"
     ]
    }
   ],
   "source": [
    "print(clf.predict([[1,1,1,0]]))#预测"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 鸢尾花数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Source.gv.pdf'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn import tree\n",
    "import graphviz\n",
    "import os\n",
    "#引入数据\n",
    "iris=load_iris()\n",
    "X=iris.data\n",
    "y=iris.target\n",
    "\n",
    "#训练数据和模型,采用C4.5训练\n",
    "clf=tree.DecisionTreeClassifier(criterion='entropy')\n",
    "clf=clf.fit(X,y)\n",
    "\n",
    "#引入graphviz模块用来导出图,结果图如下所示\n",
    "import graphviz\n",
    "dot_data=tree.export_graphviz(clf,out_file=None,\n",
    "                              feature_names=iris.feature_names,\n",
    "                              class_names=iris.target_names,\n",
    "                              filled=True,rounded=True,\n",
    "                              special_characters=True)\n",
    "os.chdir(r'C:\\graphviz-2.38\\release\\bin')\n",
    "graph=graphviz.Source(dot_data)\n",
    "graph.view()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 预测房价回归树"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "import graphviz\n",
    "import numpy as np\n",
    "from sklearn import tree\n",
    "# 加载boston数据函数\n",
    "boston = load_boston()\n",
    "X = boston.data\n",
    "y = boston.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8761986203607748"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "data_train, data_test, target_train, target_test =train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
    "dtr = tree.DecisionTreeRegressor(random_state = 42)\n",
    "dtr.fit(data_train, target_train)\n",
    "dtr.score(data_test, target_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参数调优"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 8, 'min_impurity_decrease': 0.0, 'min_samples_split': 30}\n",
      "0.9189831392996877\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "# 加载调优函数\n",
    "# 设置参数可取值\n",
    "gini_impure = np.linspace(0,0.01,10)\n",
    "param_grid = {\"min_impurity_decrease\":gini_impure,\"max_depth\":range(2,10),\"min_samples_split\":range(2,50,2)}\n",
    "# 设置参数网格\n",
    "reg = GridSearchCV(tree.DecisionTreeRegressor(),param_grid)\n",
    "# 建模\n",
    "reg.fit(X,y)\n",
    "# 拟合训练集数据\n",
    "print(reg.best_params_)\n",
    "print(reg.score(X,y))\n",
    "# 打印结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Source.gv.pdf'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(r'C:\\graphviz-2.38\\release\\bin')\n",
    "reg = tree.DecisionTreeRegressor(min_impurity_decrease=0,max_depth=8,min_samples_split=30)\n",
    "# 加载模型\n",
    "reg.fit(X,y)\n",
    "\n",
    "dot = tree.export_graphviz(reg,out_file=None,\n",
    "                          feature_names=boston.feature_names,\n",
    "                          filled=True,rounded=True)\n",
    "# 生成一个DOT格式的决策树\n",
    "graph = graphviz.Source(dot)\n",
    "# 使用graphviz逐字渲染dot对象\n",
    "graph.view()\n",
    "# 生成对应的图形文件boston.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from math import log\n",
    "log(2,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8949517866414867"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "5/14*(-4/5*log(4/5,2)-1/5*log(1/5,2))+9/14*(-5/9*log(5/9,2)-4/9*log(4/9,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4854268271702415"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-5/14*log(5/14,2)-9/14*log(5/14,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5904750405287549"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1.4854268271702415-0.8949517866414867"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "5/14*(-4/5*log(4/5,2)-1/5*log(1/5,2))+9/14*(-5/9*log(5/9,2)-4/9*log(4/9,2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
